# Structured Output統合設計書
## novel2manga v2.0 - 1回のLLM呼び出しで全てを解決

---

## 1. 核心コンセプト

### 1.1 現状の問題
```mermaid
graph LR
    subgraph 現状：複数回呼び出し
        A[チャンク] --> B[要約生成]
        A --> C[エンティティ抽出]
        A --> D[セリフ抽出]
        A --> E[シーン分析]
        A --> F[スクリプト変換]
    end

    B --> G[5回のLLM呼び出し<br/>合計15,000トークン]
    C --> G
    D --> G
    E --> G
    F --> G
```

### 1.2 新アプローチ：統合Structured Output
```mermaid
graph LR
    subgraph 新方式：1回呼び出し
        H[チャンク] --> I[統合分析<br/>Structured Output]
        I --> J[全情報を<br/>1つのJSONで取得<br/>3,000トークン]
    end
```

---

## 2. 統合スキーマ設計

### 2.1 マスタースキーマ（全情報を1つに）

```typescript
// これ1つで全てを表現
export const IntegratedChunkAnalysisSchema = z.object({
  // ===== エンティティ識別 =====
  entities: z.object({
    // 既知エンティティの更新
    known: z.array(z.object({
      id: z.string(),                    // "char_001"
      mentioned_as: z.array(z.string()),  // ["太郎", "たろう", "彼"]
      new_info: z.string().optional(),    // 新たに判明した情報（50文字以内）
      actions: z.array(z.string()),       // このチャンクでの行動（各20文字以内）
      confidence: z.number()               // 0.0-1.0
    })),

    // 新規エンティティ
    new: z.array(z.object({
      temp_id: z.string(),                // "temp_char_1"
      names: z.array(z.string()),         // ["花子", "はなちゃん"]
      description: z.string(),             // 初登場時の説明（50文字以内）
      role: z.enum(['major', 'minor', 'background']),
      possible_match: z.object({          // 既存キャラの可能性
        id: z.string(),
        confidence: z.number()
      }).optional()
    }))
  }),

  // ===== 構造要素（スクリプト変換用）=====
  structure: z.object({
    scenes: z.array(z.object({
      location: z.string(),               // "学校の屋上"
      time: z.string().optional(),        // "昼休み"
      atmosphere: z.string(),             // "緊張感のある"（10文字以内）
      span: z.array(z.number())          // [開始文字位置, 終了文字位置]
    })),

    dialogues: z.array(z.object({
      speaker_id: z.string(),            // "char_001" or "temp_char_1"
      text: z.string(),                  // セリフ本文
      emotion: z.string(),               // "喜び"（5文字以内）
      type: z.enum(['speech', 'thought', 'shout', 'whisper']),
      position: z.number()               // テキスト内位置
    })),

    narrations: z.array(z.object({
      type: z.enum(['description', 'action', 'internal']),
      text: z.string(),                  // ナレーション（100文字以内）
      importance: z.number()             // 1-5
    }))
  }),

  // ===== イベントと関係性 =====
  events: z.object({
    key_events: z.array(z.string()),    // 重要な出来事（各30文字以内、最大3個）

    relationships: z.array(z.object({
      from_id: z.string(),
      to_id: z.string(),
      type: z.enum(['friendly', 'hostile', 'romantic', 'family', 'professional']),
      description: z.string()            // "兄妹"（10文字以内）
    })),

    emotional_beats: z.array(z.object({
      type: z.enum(['tension', 'relief', 'climax', 'transition']),
      intensity: z.number(),            // 1-5
      position: z.number()
    }))
  }),

  // ===== メタ情報 =====
  meta: z.object({
    summary: z.string(),                 // チャンク要約（50文字以内）
    pacing: z.enum(['slow', 'normal', 'fast']),
    genre_hints: z.array(z.string()),   // ["romance", "mystery"]
    requires_detail: z.boolean(),       // Stage2が必要か
    confidence: z.number()               // 全体的な信頼度
  })
})

export type IntegratedChunkAnalysis = z.infer<typeof IntegratedChunkAnalysisSchema>
```

### 2.2 最小コンテキストスキーマ（入力用）

```typescript
// LLMに送る最小限のコンテキスト
export const MinimalContextSchema = z.object({
  active_entities: z.array(z.object({
    id: z.string(),
    names: z.array(z.string()).max(3),   // 主要な名前3つまで
    last_seen: z.number(),
    summary: z.string().max(30)          // 30文字の要約
  })).max(5),                            // 最大5人

  recent_events: z.array(z.string()).max(3), // 直近3イベント

  current_scene: z.object({
    location: z.string(),
    time: z.string().optional()
  }).optional(),

  story_context: z.string().max(100)     // 100文字の文脈
})
```

---

## 3. 統合プロンプト設計

### 3.1 マスタープロンプト（1回で全て）

```typescript
const INTEGRATED_ANALYSIS_PROMPT = `
あなたは小説分析の専門家です。与えられたテキストから、キャラクター、構造、イベントを同時に抽出します。

# コンテキスト
{{#if context.active_entities}}
## アクティブな登場人物（最新{{context.active_entities.length}}人）
{{#each context.active_entities}}
- {{this.id}}: {{this.names}} / {{this.summary}} ({{this.last_seen}}話前)
{{/each}}
{{/if}}

{{#if context.recent_events}}
## 直近の出来事
{{#each context.recent_events}}
- {{this}}
{{/each}}
{{/if}}

{{#if context.current_scene}}
## 現在の場面
{{context.current_scene.location}}{{#if context.current_scene.time}}、{{context.current_scene.time}}{{/if}}
{{/if}}

# 分析対象テキスト
{{chunk_text}}

# タスク
以下の要素を同時に抽出し、指定されたJSON構造で出力してください：

1. **エンティティ識別**
   - 既知の登場人物の識別（表記揺れも同一人物として認識）
   - 新規登場人物の検出
   - 各人物の新情報と行動

2. **構造要素の抽出**
   - シーン（場所、時間、雰囲気）
   - セリフ（話者、感情、種類）
   - ナレーション（種類、重要度）

3. **イベントと関係性**
   - 重要な出来事（最大3個、各30文字以内）
   - 人物間の関係性
   - 感情的な転換点

4. **メタ情報**
   - 50文字の要約
   - ペーシング判定
   - ジャンルヒント

# 重要な指示
- 「太郎」「たろう」「タロちゃん」のような表記揺れは同一人物として扱う
- 代名詞（彼、彼女）は文脈から特定
- 省略された主語も推定
- 各フィールドの文字数制限を厳守
- 不明な場合はnullではなく空配列や空文字列を使用

# 出力形式
厳密に定義されたJSONスキーマに従って出力してください。
`;
```

### 3.2 効率的なLLM呼び出し実装

```typescript
export class IntegratedAnalyzer {
  private llm: StructuredLLMClient;

  async analyzeChunk(
    chunkText: string,
    context: MinimalContext,
    chunkIndex: number
  ): Promise<IntegratedChunkAnalysis> {
    // 1回の呼び出しで全情報取得
    const response = await this.llm.createStructuredCompletion({
      model: 'gpt-4-turbo-2024-04-09', // or 'claude-3-opus'
      messages: [
        {
          role: 'system',
          content: 'あなたは小説の統合分析システムです。'
        },
        {
          role: 'user',
          content: this.buildPrompt(chunkText, context)
        }
      ],
      response_format: {
        type: 'json_schema',
        json_schema: {
          name: 'integrated_chunk_analysis',
          strict: true,
          schema: IntegratedChunkAnalysisSchema
        }
      },
      max_tokens: 2000,
      temperature: 0.3  // 低温度で一貫性重視
    });

    return response.parsed;
  }

  private buildPrompt(chunkText: string, context: MinimalContext): string {
    // Handlebarsテンプレート使用
    const template = Handlebars.compile(INTEGRATED_ANALYSIS_PROMPT);
    return template({
      chunk_text: chunkText,
      context: context
    });
  }
}
```

---

## 4. プロバイダー別実装

### 4.1 OpenAI (GPT-4-turbo with JSON mode)

```typescript
const openAICall = {
  model: "gpt-4-turbo-preview",
  messages: [...],
  response_format: {
    type: "json_object"
  },
  // 関数呼び出し形式も可能
  functions: [{
    name: "analyze_chunk",
    description: "Analyze novel chunk",
    parameters: IntegratedChunkAnalysisSchema
  }],
  function_call: { name: "analyze_chunk" }
};
```

### 4.2 Anthropic (Claude with structured output)

```typescript
const claudeCall = {
  model: "claude-3-opus-20240229",
  messages: [...],
  // XMLタグを使った構造化
  system: `Return your response as valid JSON matching this schema:
${JSON.stringify(IntegratedChunkAnalysisSchema, null, 2)}

Wrap your JSON response in <json></json> tags.`,
  max_tokens: 2000
};
```

### 4.3 Google (Gemini with response schema)

```typescript
const geminiCall = {
  model: "gemini-1.5-pro",
  contents: [...],
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: IntegratedChunkAnalysisSchema,
    temperature: 0.3,
    maxOutputTokens: 2000
  }
};
```

---

## 5. トークン最適化戦略

### 5.1 トークン使用量の詳細分析

```typescript
interface TokenBreakdown {
  // 入力トークン
  input: {
    system_prompt: 200,        // 固定
    context: 300,             // 最小限のアクティブエンティティ
    chunk_text: 1000,         // チャンクテキスト
    schema_hint: 100,         // スキーマのヒント
    total: 1600
  },

  // 出力トークン
  output: {
    structured_json: 800,     // 構造化された全情報
    total: 800
  },

  // 合計
  total: 2400,                // 従来の15,000から84%削減

  // 従来手法との比較
  comparison: {
    traditional: {
      summary: 3000,
      entity_extraction: 3000,
      dialogue_extraction: 3000,
      scene_analysis: 3000,
      script_prep: 3000,
      total: 15000
    },
    savings: 12600,          // 節約トークン数
    reduction_rate: 0.84     // 84%削減
  }
}
```

### 5.2 スキーマ最適化テクニック

```typescript
// 1. 列挙型で選択肢を制限
emotion: z.enum(['joy', 'anger', 'sadness', 'fear', 'surprise', 'neutral'])
// → 自由記述より50%トークン削減

// 2. 文字数制限で出力を制御
summary: z.string().max(50)
// → 無制限より70%削減

// 3. 配列の最大数を制限
key_events: z.array(z.string()).max(3)
// → 無制限より60%削減

// 4. ネストを浅くする
// Bad: deeply.nested.structure.with.many.levels
// Good: flat_structure_with_ids

// 5. IDによる参照
speaker_id: z.string()  // "char_001"
// → 名前の繰り返しより80%削減
```

---

## 6. エラーハンドリングとフォールバック

### 6.1 バリデーションとリトライ

```typescript
class RobustAnalyzer {
  async analyzeWithRetry(
    chunk: string,
    context: MinimalContext,
    maxRetries: number = 3
  ): Promise<IntegratedChunkAnalysis> {
    for (let attempt = 0; attempt < maxRetries; attempt++) {
      try {
        const result = await this.analyze(chunk, context);

        // スキーマバリデーション
        const validated = IntegratedChunkAnalysisSchema.safeParse(result);

        if (validated.success) {
          return validated.data;
        }

        // バリデーションエラーの場合、プロンプトを調整
        console.warn(`Validation failed (attempt ${attempt + 1}):`, validated.error);
        context = this.adjustContext(context, validated.error);

      } catch (error) {
        if (attempt === maxRetries - 1) throw error;

        // レート制限やタイムアウトの場合は待機
        await this.wait(Math.pow(2, attempt) * 1000);
      }
    }

    throw new Error('Max retries exceeded');
  }
}
```

### 6.2 部分的成功の処理

```typescript
// 必須フィールドと任意フィールドを分離
const EssentialSchema = z.object({
  entities: z.object({
    known: z.array(...).default([]),
    new: z.array(...).default([])
  }),
  meta: z.object({
    summary: z.string(),
    confidence: z.number()
  })
});

const OptionalEnhancements = z.object({
  structure: ...,
  events: ...,
}).partial();

// 段階的な処理
async function analyzeWithGracefulDegradation(chunk: string) {
  try {
    // 完全な分析を試みる
    return await fullAnalysis(chunk);
  } catch {
    // 失敗したら必須要素のみ
    return await essentialAnalysis(chunk);
  }
}
```

---

## 7. パフォーマンス最適化

### 7.1 バッチ処理とストリーミング

```typescript
class StreamingAnalyzer {
  async* analyzeChunksStreaming(
    chunks: string[],
    batchSize: number = 5
  ): AsyncGenerator<IntegratedChunkAnalysis> {
    const batches = this.createBatches(chunks, batchSize);

    for (const batch of batches) {
      // 並列処理
      const promises = batch.map((chunk, idx) =>
        this.analyzeChunk(chunk, this.getContext(idx))
      );

      const results = await Promise.allSettled(promises);

      for (const result of results) {
        if (result.status === 'fulfilled') {
          yield result.value;
        } else {
          console.error('Chunk analysis failed:', result.reason);
          yield this.createFallbackResult();
        }
      }
    }
  }
}
```

### 7.2 キャッシングとメモ化

```typescript
class CachedAnalyzer {
  private cache = new Map<string, IntegratedChunkAnalysis>();

  async analyzeWithCache(
    chunk: string,
    context: MinimalContext
  ): Promise<IntegratedChunkAnalysis> {
    // キャッシュキー生成（chunk + contextのハッシュ）
    const key = this.generateCacheKey(chunk, context);

    // キャッシュヒット
    if (this.cache.has(key)) {
      return this.cache.get(key)!;
    }

    // 新規分析
    const result = await this.analyze(chunk, context);

    // キャッシュ保存（LRU戦略）
    if (this.cache.size >= 100) {
      const firstKey = this.cache.keys().next().value;
      this.cache.delete(firstKey);
    }
    this.cache.set(key, result);

    return result;
  }
}
```

---

## 8. 実装例：完全な処理パイプライン

```typescript
export class NovelAnalysisPipeline {
  private analyzer: IntegratedAnalyzer;
  private store: EntityStore;

  async processNovel(text: string): Promise<NovelAnalysisResult> {
    const chunks = this.splitIntoChunks(text);
    const results: IntegratedChunkAnalysis[] = [];

    for (const [index, chunk] of chunks.entries()) {
      // 1. 最小限のコンテキスト準備
      const context = this.prepareMinimalContext(index);

      // 2. 統合分析（1回のLLM呼び出し）
      const analysis = await this.analyzer.analyzeChunk(
        chunk,
        context,
        index
      );

      // 3. エンティティストア更新
      this.updateEntityStore(analysis, index);

      // 4. 必要に応じて詳細分析
      if (analysis.meta.requires_detail) {
        await this.performDetailedAnalysis(chunk, analysis);
      }

      results.push(analysis);

      // 5. 進捗とメトリクス
      this.reportProgress(index, chunks.length);
      this.recordMetrics(analysis);
    }

    return this.aggregateResults(results);
  }

  private prepareMinimalContext(chunkIndex: number): MinimalContext {
    // アクティブエンティティ（最大5個）
    const activeEntities = this.store
      .getActiveEntities(chunkIndex)
      .slice(0, 5)
      .map(e => ({
        id: e.id,
        names: e.names.slice(0, 3),
        last_seen: chunkIndex - e.lastSeenChunk,
        summary: e.summary.substring(0, 30)
      }));

    // 直近イベント（最大3個）
    const recentEvents = this.store
      .getRecentEvents(chunkIndex, 3)
      .map(e => e.substring(0, 50));

    return {
      active_entities: activeEntities,
      recent_events: recentEvents,
      current_scene: this.store.getCurrentScene(chunkIndex),
      story_context: this.store.getStoryContext(100)
    };
  }
}
```

---

## 9. 期待される成果

### 9.1 定量的効果

| 指標 | 従来手法 | Structured Output | 改善率 |
|------|----------|-------------------|--------|
| LLM呼び出し回数/チャンク | 5回 | 1回 | 80%削減 |
| トークン使用量/チャンク | 15,000 | 2,400 | 84%削減 |
| 処理時間/チャンク | 30秒 | 8秒 | 73%短縮 |
| API費用/100チャンク | $50 | $8 | 84%削減 |
| エラー率 | 5% | 1% | 80%改善 |

### 9.2 定性的効果

1. **一貫性の向上**
   - 1回の分析で全要素を同時に考慮
   - コンテキストの統一的な解釈

2. **開発効率**
   - プロンプト管理の簡素化
   - デバッグの容易さ

3. **拡張性**
   - 新しい分析要素の追加が容易
   - スキーマの段階的な進化

---

## 10. まとめ：実装への道筋

### Phase 1: スキーマ定義とテスト（Week 1）
```typescript
// 1. スキーマの定義
const schema = IntegratedChunkAnalysisSchema;

// 2. サンプルデータでテスト
const testResult = schema.safeParse(sampleData);

// 3. 調整と最適化
```

### Phase 2: プロンプト開発（Week 2）
```typescript
// 1. 基本プロンプトの作成
// 2. 各プロバイダーでのテスト
// 3. 精度とトークン使用量の測定
```

### Phase 3: 統合実装（Week 3-4）
```typescript
// 1. パイプライン実装
// 2. エラーハンドリング
// 3. パフォーマンス最適化
```

### Phase 4: 本番展開（Week 5-6）
```typescript
// 1. A/Bテスト
// 2. 段階的ロールアウト
// 3. モニタリングとチューニング
```

