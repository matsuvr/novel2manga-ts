# novel2manga-ts: トークン消費削減戦略

拝見しました。超長文小説の一貫性を保ちながら、LLMのトークン消費を抑えるという課題、非常に興味深いです。現状のアーキテクチャは「各チャンクの分析結果を次のチャンクにすべて引き継ぐ」ことで一貫性を担保しており、これがトークン数増大の根本原因ですね。

ご提案いただいた要件「登場人物や場面の一貫性保持」を維持しつつ、トークン消費を劇的に削減するための新しいロジックを2段階で提案します。即時的な改善と、将来的なスケーラビリティを確保するための抜本的な改善です。

### 提案の概要

根本的な解決策は、**「コンテキスト（文脈）の全体受け渡し」から「動的なコンテキスト検索」へ移行する**ことです。各チャンクの処理に必要な情報だけを、巨大なメモリの海から的確に取得してプロンプトに注入します。

---

### 提案1：アクティブコンテキスト管理（短期的な改善）

これは、現在の実装に比較的少ない変更で導入でき、即時的なトークン削減効果が期待できる方法です。

**コンセプト：**
全キャラクターの情報を常に引き回すのではなく、「そのチャンクに関連する可能性が高いキャラクター」の情報だけをプロンプトに含めます。

**具体的なロジック：**

1.  **キャラクターメモリの拡張：**
    まず、キャラクター情報を管理するJSONに、`lastSeenChunk`（最後に登場したチャンク番号）と `mentionCount`（総登場回数）のようなメタデータを追加します。

2.  **事前スキャン：**
    LLMにチャンクを渡す前に、まずチャンクのテキストから登場しそうなキャラクター名やエイリアスを単純な文字列検索でリストアップします。

3.  **コンテキストの動的構築：**
    `textAnalysis`のプロンプトを生成する際に、`previousCharacterMemoryJson`を以下のようにフィルタリングします。
    *   **アクティブキャラクター：** 事前スキャンでヒットしたキャラクターや、直近 `N` チャンク（例：3チャンク）以内に登場したキャラクターは、全情報をプロンプトに含めます。
    *   **インアクティブキャラクター：** それ以外のキャラクターは、`{ "id": "char_XX", "name": "名前" }` のような最小限の情報のみ、あるいはプロンプトから完全に除外します。

4.  **累積的な情報更新：**
    `textAnalysis`の `description` の役割を「このチャンクで新たに判明した情報」から「**これまでの情報をすべて統合した累積的な説明**」に変更します。これにより、過去の情報を参照しなくても、そのキャラクターの最新の状況がわかるようになります。

**メリット：**
*   実装が比較的容易。
*   `previousCharacterMemoryJson` が肥大化するのを防ぎ、即座にトークン数を削減できる。

**デメリット：**
*   文字列検索にヒットしない間接的な言及（「彼」「あの男」など）や、文脈からのみ推測できる関連情報を取りこぼす可能性がある。

---

### 提案2：セマンティック・メモリ・データベース（長期的・抜本的改善）

これは、より高度でスケーラブルなアプローチです。ベクトルデータベースを利用して、意味的に関連する情報を検索し、コンテキストとして利用します。

**コンセプト：**
小説から抽出したすべての情報（キャラクター、シーン、イベント等）を「記憶の断片」としてベクトル化し、データベースに保存します。チャンクを処理する際は、そのチャンクの内容と意味的に最も近い「記憶の断片」を検索し、プロンプトに埋め込みます。

**具体的なロジック：**

1.  **情報のベクトル化とインデックス作成：**
    `textAnalysis`が完了するたびに、抽出された各要素（`characters`, `scenes`, `highlights`など）を個別のドキュメントとして扱います。
    *   例：`"ドキュメント1": "登場人物: 太郎。新たに判明した情報: 彼は復讐のために旅をしている。"`
    *   これらのドキュメントを埋め込みモデル（Embedding Model）でベクトル化し、ベクトルデータベース（例: ChromaDB, Pinecone, Faiss）に保存します。

2.  **意味検索によるコンテキスト取得：**
    新しいチャンクを処理する際（`textAnalysis`または`scriptConversion`の前）、まずチャンクのテキスト自体をベクトル化します。
    *   そのベクトルを使ってベクトルデータベースを検索し、類似度が最も高い `Top-K`（例: 5〜10件）の「記憶の断片」を取得します。

3.  **超圧縮プロンプトの生成：**
    取得した `Top-K` の情報のみを、`[関連記憶]` のような形でプロンプトの冒頭に注入します。

    **新しい`userPromptTemplate`の例：**
    ```
    [関連記憶]
    - 登場人物 "太郎" はチャンク5で「影の剣」を手に入れた。
    - 場面: "呪われた森" は魔法が不安定になる場所である。
    - 以前の出来事: チャンク8で、太郎は旧友の裏切りにあった。

    [分析対象チャンク]
    {{chunkText}}

    [指示]
    上記の[関連記憶]を参考に、[分析対象チャンク]を分析し、JSONを出力してください...
    ```

**メリット：**
*   **トークン数が完全に一定に：** 小説の長さに全く影響されず、プロンプトサイズが `K` 個の関連情報分に固定されます。
*   **高精度なコンテキスト：** 単なる文字列マッチングではなく、意味的な関連性に基づいているため、「彼」のような代名詞や、直接言及されていない過去の出来事の影響なども捉えることができます。
*   **究極のスケーラビリティ：** どれだけ小説が長くなっても、処理速度とコストが安定します。

**デメリット：**
*   ベクトルデータベースという新しい技術要素の導入が必要。
*   埋め込みモデルの選択や、検索精度のチューニングなど、初期開発コストがやや高い。

---

### プロンプト自体の最適化案

上記に加えて、現在のプロンプトをさらに効率化する案です。

1.  **JSONキーの短縮：**
    `textAnalysis`のJSONスキーマのキーを短くするだけでも、トークン削減に繋がります。
    *   `firstAppearanceChunk` -> `newChunk`
    *   `possibleMatchIds` -> `matches`
    *   `characterEvents` -> `events`

2.  **タスクの分割（Chain of Thoughtの細分化）：**
    `textAnalysis`という一つの大きなタスクを、より小さなLLMコールに分割します。
    *   **ステップ1（エンティティ認識）：** チャンクテキストと最小限のキャラクターリストを渡し、「このチャンクに登場するキャラクターIDと、新規キャラクターの候補名」だけを抽出させる。
    *   **ステップ2（イベント抽出）：** ステップ1で特定したキャラクター情報だけを使い、「シーン」「ハイライト」「セリフ」などを抽出させる。これにより、各LLMの責務が明確になり、プロンプトを簡潔にできます。

### 推奨する進め方

1.  **まず「提案1：アクティブコンテキスト管理」を実装する。**
    これにより、比較的少ない工数で、現状の課題を大きく改善できます。

2.  **次に「提案2：セマンティック・メモリ・データベース」の導入を検討する。**
    これを将来的なアーキテクチャのゴールと位置づけることで、サービスの長期的な安定性と品質向上に繋がります。

これらの提案が、`novel2manga-ts` のさらなる発展の一助となれば幸いです。
